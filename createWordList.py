# -*- coding: utf-8 -*-
"""append_not.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FSCsVrbQvhDZVrX4P0sZGO9SO6Ag-tvr
"""
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import wordpunct_tokenize
import string
import re
from nltk.stem.lancaster import LancasterStemmer

#Dicts of contractions
contractions = {
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he's": "he is",
"how'd": "how did",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'll": "i will",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'll": "it will",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"must've": "must have",
"mustn't": "must not",
"needn't": "need not",
"oughtn't": "ought not",
"shan't": "shall not",
"sha'n't": "shall not",
"she'd": "she would",
"she'll": "she will",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"that'd": "that would",
"that's": "that is",
"there'd": "there had",
"there's": "there is",
"they'd": "they would",
"they'll": "they will",
"they're": "they are",
"they've": "they have",
"wasn't": "was not",
"we'd": "we would",
"we'll": "we will",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"where'd": "where did",
"where's": "where is",
"who'll": "who will",
"who's": "who is",
"won't": "will not",
"wouldn't": "would not",
"you'd": "you would",
"you'll": "you will",
"you're": "you are"
}

#Remove punctuation in a list
pattern = re.compile(r'[\W_]+')
def strip_token(token):
    token = pattern.sub('', token).strip()
    if len(token): return token
    return None

#Remove None tokens in a list
def remove_none(lista):
    lista = list(filter(None, lista))
    return lista

#de-contract words
def clean(text):
  text = text.split()
  new_text = []
  for word in text:
    if word in contractions:
      new_text.append(contractions[word])
    else:
      new_text.append(word)
  text = " ".join(new_text)
  return text

#df = pd.read_csv('reviews_small_afinn.csv')

#Importing Amazon reviews Dataset
h = ['productid', 'userid', 'score', 'text']
df = pd.read_csv('food.tsv', sep = '\t', skiprows = 1, header = None, encoding = 'latin-1', names = h)

#Convert all reviews in lower case
text = df['text']
df['text'] = list(map(lambda x: x.lower(), text))

#Cleaning text using function to remove contractions
clean_text = []
for text in df.text:
   clean_text.append(clean(text))
df['clean_text'] = clean_text

text = df['clean_text']

#Tokenizing every reviews
df['tokenized'] = df.apply(lambda row: wordpunct_tokenize(row['clean_text']), axis=1)

tokenized = df['tokenized']

#Adding stopwords
stop = stopwords.words('english')
stop = set(stop)
stop.remove('not')
stop.add("''")
stop.add("'s'")
stop.add("p.s")
stop.add("...")
stop.add('``')
stop.add("br")


'''
text_tokenized_stop = []
for sentence in tokenized:
    sublist = []
    for subsent in sentence:
        el = []
        for element in subsent:
            if(element not in stop):
                el.append(element)
        sublist.append(el)
    text_tokenized_stop.append(sublist)
'''

#Removing stopwords except not
tokenized_stop = tokenized.apply(lambda x: [item for item in x if item not in stop])

#remove puntaction
punctuation = string.punctuation
tokenized_stop_punct = []
for sentence in tokenized_stop:
    el = []
    for element in sentence:
        element = strip_token(element)

        '''
        if(element not in punctuation):
        '''
        el.append(element)
    el = list(filter(None, el))
    el = list(filter(lambda x: not x.isdigit(), el))
    tokenized_stop_punct.append(el)


#Concatenating token["not"] and token[generic word] if token["not"] comes before a token[generic word]
#Every token["not"] at the and of a sentece will be removed later
count = 0
for row in tokenized_stop_punct:
  new_row = []
  b = False
  for word in row:
    if(b):
      new_row.append(str('not'+word))
      b = False
    elif(word == 'not'):
      b = True
    else:
      new_row.append(word)
  tokenized_stop_punct[count] = new_row
  count = count + 1

#Adding the "not" word to stopwords again
#This is why we have to delete the last token["not"] left at the end of sentences
stop.add('not')
final_tokenized = list(map(lambda x: list(filter(lambda x: x not in stop, x)), tokenized_stop_punct))

#Create object stemmer
lancaster_stemmer = LancasterStemmer()

#Performing stemming
final_tokenized_stem = list([lancaster_stemmer.stem(item) for sentence in final_tokenized for item in sentence])

#Creating sorted list with no repetitions cotanining all stemmed words of the dataset
final_tokenized_stem = set(final_tokenized_stem)
final_tokenized_stem = list(final_tokenized_stem)
final_tokenized_stem = sorted(final_tokenized_stem)

#Creating WordList.txt
with open('WordList.txt', 'w') as outfile:
  for word in final_tokenized_stem:
    outfile.write(word+'\n')
