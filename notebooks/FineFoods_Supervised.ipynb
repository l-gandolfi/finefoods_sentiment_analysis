{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineFoods_Supervised.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7Osg9L3ZPcN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "dd462fca-09c7-4ec3-e061-367938fe7d63"
      },
      "source": [
        "!pip install afinn\n",
        "!pip install vaderSentiment\n",
        "!pip install sklearn\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n",
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.2.1\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.13.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0QLfkqQaNq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1ad44edc-55ce-4f03-88e2-d52e1900b03f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from afinn import Afinn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk import wordpunct_tokenize\n",
        "\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fqkFKGgaTwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "08bfd3dd-b358-47fe-fbe6-d8f48b410611"
      },
      "source": [
        "#leggo il dataset con pandas\n",
        "df = pd.read_csv('reviews_small_timed.csv')\n",
        "\n",
        "#Convert all reviews in lower case\n",
        "text = df['text']\n",
        "df['text'] = list(map(lambda x: x.lower(), text))\n",
        "df.head()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>productid</th>\n",
              "      <th>userid</th>\n",
              "      <th>profile</th>\n",
              "      <th>helpfulNum</th>\n",
              "      <th>helpfulDen</th>\n",
              "      <th>score</th>\n",
              "      <th>text</th>\n",
              "      <th>time</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>2011-04-27</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>2012-09-07</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>2008-08-18</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>2011-06-13</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>great taffy at a great price.  there was a wid...</td>\n",
              "      <td>2012-10-21</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    productid          userid  ...        time  year\n",
              "0  B001E4KFG0  A3SGXH7AUHU8GW  ...  2011-04-27  2011\n",
              "1  B00813GRG4  A1D87F6ZCVE5NK  ...  2012-09-07  2012\n",
              "2  B000LQOCH0   ABXLMWJIXXAIN  ...  2008-08-18  2008\n",
              "3  B000UA0QIQ  A395BORC6FGVXV  ...  2011-06-13  2011\n",
              "4  B006K2ZZ7K  A1UQRSCLF8GW1T  ...  2012-10-21  2012\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFx0Wy1RblSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "dd33fb6d-ae15-4021-97c5-4f137f354935"
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}\n",
        "\n",
        "#de-contract words\n",
        "def clean(text):\n",
        "  text = text.split()\n",
        "  new_text = []\n",
        "  for word in text:\n",
        "    if word in contractions:\n",
        "      new_text.append(contractions[word])\n",
        "    else:\n",
        "      new_text.append(word)\n",
        "  text = \" \".join(new_text)\n",
        "  return text\n",
        "      \n",
        "clean_text = []\n",
        "for text in df.text:\n",
        "   clean_text.append(clean(text))\n",
        "df['clean_text'] = clean_text\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>productid</th>\n",
              "      <th>userid</th>\n",
              "      <th>profile</th>\n",
              "      <th>helpfulNum</th>\n",
              "      <th>helpfulDen</th>\n",
              "      <th>score</th>\n",
              "      <th>text</th>\n",
              "      <th>time</th>\n",
              "      <th>year</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>2011-04-27</td>\n",
              "      <td>2011</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>2012-09-07</td>\n",
              "      <td>2012</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>2008-08-18</td>\n",
              "      <td>2008</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>2011-06-13</td>\n",
              "      <td>2011</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>great taffy at a great price.  there was a wid...</td>\n",
              "      <td>2012-10-21</td>\n",
              "      <td>2012</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    productid  ...                                         clean_text\n",
              "0  B001E4KFG0  ...  i have bought several of the vitality canned d...\n",
              "1  B00813GRG4  ...  product arrived labeled as jumbo salted peanut...\n",
              "2  B000LQOCH0  ...  this is a confection that has been around a fe...\n",
              "3  B000UA0QIQ  ...  if you are looking for the secret ingredient i...\n",
              "4  B006K2ZZ7K  ...  great taffy at a great price. there was a wide...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J7aAXXbbq-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "251b500a-98bb-4894-86ff-68e20d1cfa27"
      },
      "source": [
        "#Let's tokenize the reviews using word_tokenize from nltk\n",
        "df['tokenized'] = df.apply(lambda row: word_tokenize(row['clean_text']), axis=1)\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>productid</th>\n",
              "      <th>userid</th>\n",
              "      <th>profile</th>\n",
              "      <th>helpfulNum</th>\n",
              "      <th>helpfulDen</th>\n",
              "      <th>score</th>\n",
              "      <th>text</th>\n",
              "      <th>time</th>\n",
              "      <th>year</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>2011-04-27</td>\n",
              "      <td>2011</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>[i, have, bought, several, of, the, vitality, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>2012-09-07</td>\n",
              "      <td>2012</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>[product, arrived, labeled, as, jumbo, salted,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>2008-08-18</td>\n",
              "      <td>2008</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>[this, is, a, confection, that, has, been, aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>2011-06-13</td>\n",
              "      <td>2011</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>[if, you, are, looking, for, the, secret, ingr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>great taffy at a great price.  there was a wid...</td>\n",
              "      <td>2012-10-21</td>\n",
              "      <td>2012</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "      <td>[great, taffy, at, a, great, price, ., there, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    productid  ...                                          tokenized\n",
              "0  B001E4KFG0  ...  [i, have, bought, several, of, the, vitality, ...\n",
              "1  B00813GRG4  ...  [product, arrived, labeled, as, jumbo, salted,...\n",
              "2  B000LQOCH0  ...  [this, is, a, confection, that, has, been, aro...\n",
              "3  B000UA0QIQ  ...  [if, you, are, looking, for, the, secret, ingr...\n",
              "4  B006K2ZZ7K  ...  [great, taffy, at, a, great, price, ., there, ...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BgCpQhSbuEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9cc1dc5-0d94-4518-b679-d292477f76ed"
      },
      "source": [
        "#show the tokens\n",
        "tokenized = df['tokenized']\n",
        "tokenized"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [i, have, bought, several, of, the, vitality, ...\n",
              "1        [product, arrived, labeled, as, jumbo, salted,...\n",
              "2        [this, is, a, confection, that, has, been, aro...\n",
              "3        [if, you, are, looking, for, the, secret, ingr...\n",
              "4        [great, taffy, at, a, great, price, ., there, ...\n",
              "5        [i, got, a, wild, hair, for, taffy, and, order...\n",
              "6        [this, saltwater, taffy, had, great, flavors, ...\n",
              "7        [this, taffy, is, so, good, ., it, is, very, s...\n",
              "8        [right, now, i, am, mostly, just, sprouting, t...\n",
              "9        [this, is, a, very, healthy, dog, food, ., goo...\n",
              "10       [i, do, not, know, if, it, is, the, cactus, or...\n",
              "11       [one, of, my, boys, needed, to, lose, some, we...\n",
              "12       [my, cats, have, been, happily, eating, felida...\n",
              "13       [good, flavor, !, these, came, securely, packe...\n",
              "14       [the, strawberry, twizzlers, are, my, guilty, ...\n",
              "15       [my, daughter, loves, twizzlers, and, this, sh...\n",
              "16       [i, love, eating, them, and, they, are, good, ...\n",
              "17       [i, am, very, satisfied, with, my, twizzler, p...\n",
              "18       [twizzlers, ,, strawberry, my, childhood, favo...\n",
              "19       [candy, was, delivered, very, fast, and, was, ...\n",
              "20       [my, husband, is, a, twizzlers, addict, ., we,...\n",
              "21       [i, bought, these, for, my, husband, who, is, ...\n",
              "22       [i, can, remember, buying, this, candy, as, a,...\n",
              "23       [i, love, this, candy, ., after, weight, watch...\n",
              "24       [i, have, lived, out, of, the, us, for, over, ...\n",
              "25       [product, received, is, as, advertised., <, br...\n",
              "26       [the, candy, is, just, red, ,, no, flavor, ., ...\n",
              "27       [i, was, so, glad, amazon, carried, these, bat...\n",
              "28       [i, got, this, for, my, mum, who, is, not, dia...\n",
              "29       [i, do, not, know, if, it, is, the, cactus, or...\n",
              "                               ...                        \n",
              "99970    [if, you, like, spicy, ramen, noodles, ,, thes...\n",
              "99971    [it, is, the, same, taste, as, the, one, i, bo...\n",
              "99972    [first, of, all, ,, kudos, to, the, super, sav...\n",
              "99973    [i, love, these, ., it, is, hard, to, think, o...\n",
              "99974    [i, will, preface, this, to, say, that, i, lik...\n",
              "99975    [this, is, the, best, noodle, i, have, ever, h...\n",
              "99976    [i, was, browsing, amazon, when, i, came, acro...\n",
              "99977    [this, is, the, best, tasting, ,, artery, clog...\n",
              "99978    [noodles, are, fine, and, filling, ., they, ma...\n",
              "99979    [my, first, exposure, to, shin, ramyun, was, a...\n",
              "99980    [i, bought, this, product, strictly, on, the, ...\n",
              "99981    [it, is, hot, and, delicious, ., the, noodle, ...\n",
              "99982    [what, can, i, say, ?, they, are, very, spicy,...\n",
              "99983    [i, have, purchased, this, noodle, from, local...\n",
              "99984    [this, stuff, is, awesome, !, it, has, the, ri...\n",
              "99985    [just, about, any, nong, shim, soup, is, good,...\n",
              "99986    [i, bought, these, on, a, whim, because, they,...\n",
              "99987    [apparently, they, used, too, much, synthetic,...\n",
              "99988    [stacks, of, these, shin, ramyun, are, stored,...\n",
              "99989    [i, been, to, several, places, around, the, wo...\n",
              "99990    [this, item, is, really, over, price, ,, i, ca...\n",
              "99991    [i, would, have, to, say, the, noodles, packag...\n",
              "99992    [i, originally, decided, to, purchase, this, p...\n",
              "99993    [this, stuff, is, awesome, ., for, best, flavo...\n",
              "99994    [i, love, these, noodle, ., a, little, to, spi...\n",
              "99995    [i, just, love, it, and, will, buy, another, b...\n",
              "99996    [my, late, father, in, law, used, to, have, a,...\n",
              "99997    [this, is, my, favorite, brand, of, korean, ra...\n",
              "99998    [i, do, like, these, noodles, although, ,, to,...\n",
              "99999    [i, love, this, noodle, and, have, it, once, o...\n",
              "Name: tokenized, Length: 100000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFwazytdccm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a flat list\n",
        "sentences = (list(itertools.chain(tokenized)))\n",
        "flat_list = [item for sublist in sentences for item in sublist]\n",
        "\n",
        "#remove stopwords\n",
        "stop =stopwords.words('english')\n",
        "#add stopwords\n",
        "stop =set(stop)\n",
        "stop.add(\"br\")\n",
        "text_tokenized_stop = tokenized.apply(lambda x: [item for item in x if item not in stop])\n",
        "sentences = (list(itertools.chain(text_tokenized_stop)))\n",
        "flat_list = [item for sublist in sentences for item in sublist]\n",
        "\n",
        "#remove puntaction\n",
        "punctuation = string.punctuation\n",
        "text_tokenized_stop_punct = text_tokenized_stop.apply(lambda x: [item for item in x if item not in punctuation])\n",
        "sentences = (list(itertools.chain(text_tokenized_stop_punct)))\n",
        "flat_list = [item for sublist in sentences for item in sublist]\n",
        "\n",
        "#apply stemming\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "text_tokenized_stem = text_tokenized_stop_punct.apply(lambda x: [lancaster_stemmer.stem(item) for item in x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTRsGrX7aekT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "5c7a28b6-4975-4eee-b5a1-2cd88bc218ad"
      },
      "source": [
        "count_vect = CountVectorizer(stop_words=None, lowercase=True, max_features=5000)\n",
        "bow = count_vect.fit_transform(text_tokenized_stem.apply(lambda x: \" \".join(x)))\n",
        "print(len(text_tokenized_stem))\n",
        "print(bow.shape)\n",
        "print(bow.toarray())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "(100000, 5000)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weReRvoreOPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's split into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test  = train_test_split(\n",
        "        bow.toarray(), \n",
        "        df.score,\n",
        "        train_size=0.80, \n",
        "        random_state=1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ_N-6w-enhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d84d3007-2850-421e-a86a-fd5c1f74a070"
      },
      "source": [
        "#Build the linear classifier using logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_model = LogisticRegression()\n",
        "\n",
        "#let's train with a fit\n",
        "log_model = log_model.fit(X=X_train, y=y_train)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaT1ypf5fnPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdb70b95-48d1-428c-981e-b0c5ded9a60c"
      },
      "source": [
        "#Now we can label the evaluation set\n",
        "y_pred = log_model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5 3 5 ... 5 5 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ak2VTuLgK9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c29c934-da49-45ee-d7cf-33241ce1192d"
      },
      "source": [
        "#Now let's measure our performances\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuCQGfezgXlL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "da0d6cc2-be58-412b-80b6-42e908dec2c1"
      },
      "source": [
        "#Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1089   120    83    61   512]\n",
            " [  293   180   147   100   443]\n",
            " [  153   117   386   262   748]\n",
            " [   78    62   172   635  1913]\n",
            " [  156    63   115   392 11720]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDekRrUXggvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7c90d337-a6d0-4705-f4e9-e42b6f129d61"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision, recall, f_score, support = precision_recall_fscore_support(y_test, y_pred)\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(f_score)\n",
        "print(support)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.61560204 0.33210332 0.42746401 0.43793103 0.76421492]\n",
            "[0.58391421 0.15477214 0.23169268 0.22202797 0.94166801]\n",
            "[0.59933957 0.2111437  0.30050603 0.29466357 0.84371176]\n",
            "[ 1865  1163  1666  2860 12446]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b65Hj0gQg2CW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7227e6c2-75fa-45f1-b038-0ec1d1a5a9e5"
      },
      "source": [
        "precision, recall, f_score, support = precision_recall_fscore_support(y_test, y_pred, average = \"macro\")\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(f_score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5154630637135746\n",
            "0.4268150010027381\n",
            "0.4498729256221311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaWCNqCUhCYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "da636722-4503-43f3-d720-a8b756b7c865"
      },
      "source": [
        "#La media pesata permette di ottenere risultati migliori, cuciti sul size della classe\n",
        "precision, recall, f_score, support = precision_recall_fscore_support(y_test, y_pred, average = \"weighted\")\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(f_score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.650519531938951\n",
            "0.7005\n",
            "0.6603772900171673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojlizfghHUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}